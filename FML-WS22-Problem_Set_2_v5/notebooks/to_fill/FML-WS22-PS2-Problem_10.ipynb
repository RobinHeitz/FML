{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "486faa51eccb2b8ba0e99ed360f8268b",
     "grade": false,
     "grade_id": "cell-646c0005438fc458",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Fundamentals of Machine Learning - WS 22\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0303582416cc72418a913211447ffaea",
     "grade": false,
     "grade_id": "cell-6fa7ccb43ba3aa98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# PS2#10 - The Gauss-Markov theorem\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Last updated: 2022-11-16</div>\n",
    "\n",
    "This notebook requires `numpy` and `matplotlib` to run. If the following cell raises an error, install them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6) # Default figsize\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The purpose of this notebook is to illustrate the Gauss-Markov theorem, which can be stated as follows.\n",
    "\n",
    "Let $Y$ be a random variable taking values in $\\mathbb{R}^N$ such that $$Y = \\Phi\\cdot \\bar w + \\epsilon,$$ where $\\bar w\\in\\mathbb{R}^{M+1}$ is an unknown weights vector, $\\Phi$ is a known feature matrix, and $\\epsilon$ is a noise random variable that satisfies \n",
    "1. $\\mathbb{E}[\\epsilon] = 0$;\n",
    "2. $\\mathrm{Var}[\\epsilon] = \\sigma^2\\cdot I$.\n",
    "\n",
    "Define the mean-square error of an estimator $w$ of $\\bar w$ as $$\\mathrm{MSE}[w] = \\mathbb{E}[\\lVert w - \\bar w\\rVert^2]$$\n",
    "Finally, let $$w = C\\cdot Y$$ be a linear, unbiased estimator (LUE) of $\\bar w$, and let $$w_\\mathrm{LS} = (\\Phi^\\top\\Phi)^{-1}\\Phi^\\top \\cdot Y$$ be the least-squares estimator of $\\bar w$.\n",
    "Then, $$\\mathrm{MSE}[w_\\mathrm{LS}] \\leq \\mathrm{MSE}[w].$$\n",
    "In other words, the least-squares estimator is the best linear, unbiased estimator (BLUE) of $\\bar w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustration\n",
    "\n",
    "We illustrate it by comparing the performance of the least squares estimator $w_\\mathrm{LS}$ against that of the regularized least-squares estimator $w_\\lambda$, defined as $$w_\\lambda = (\\Phi^\\top\\Phi+\\lambda\\cdot I)^{-1}\\Phi^\\top\\cdot Y.$$\n",
    "\n",
    "We consider an example with two-dimensional weigths, with ground truth $\\bar w = (1~1)^\\top$.\n",
    "We constuct $\\Phi$ by assuming that we have $\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\in\\R$ data points, and the $n^\\text{th}$-row of $\\Phi$ is $(1~x_n)$.\n",
    "Additionally, we assume that $\\epsilon$ is Gaussian with variance $0.1$.\n",
    "\n",
    "To sum up, we have the following ground truth for $Y$: $$y_n = 1 + x_n + \\mathcal{N}(0, 0.1).$$\n",
    "\n",
    "The following cell plots a two realizations of $Y$ with $N = 50$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "a = -1\n",
    "b = 1\n",
    "D = 2\n",
    "wbar = np.ones(D, dtype=float)\n",
    "sigma2 = 1\n",
    "X = np.random.uniform(a, b, (N, D-1))\n",
    "Phi = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "PhiTPhi = Phi.T @ Phi\n",
    "\n",
    "def get_realization():\n",
    "    Y = Phi @ wbar + np.random.normal(0, sigma2, N)\n",
    "    return Y\n",
    "\n",
    "if D == 2:\n",
    "    X_plot = np.linspace(a, b, 1001)\n",
    "    Y_plot = 1 + X_plot\n",
    "\n",
    "    plt.scatter(X, get_realization(), label='Realization 1', marker='x')\n",
    "    plt.scatter(X, get_realization(), label='Realization 2', marker='x')\n",
    "    plt.plot(X_plot, Y_plot, label='Truth (unknown)', color='green')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    _=plt.grid(True)\n",
    "else:\n",
    "    print(X.shape, get_realization().shape, wbar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the regularized and unregularized least-squares estimates as a function of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_estimate(y, reg):\n",
    "    b = Phi.T @ y\n",
    "    estimate = np.linalg.solve(PhiTPhi + reg * np.eye(D), b)\n",
    "    return estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For different realizations of $Y$, we get different realization of the estimates. The following cell plots the different realizations of both the unregularized and regularized estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_realizations_of_ls_estimates(R, regularizers):\n",
    "    regularizers = np.atleast_1d(regularizers)\n",
    "    L = regularizers.shape[0]\n",
    "    w = np.zeros((L, R, D), dtype=float)\n",
    "    y = np.zeros((R, N), dtype=float)\n",
    "\n",
    "    for r in range(R):\n",
    "        y[r] = get_realization()\n",
    "        for n, reg in enumerate(regularizers):\n",
    "            w[n, r] = ls_estimate(y[r], reg)\n",
    "    return w\n",
    "\n",
    "def plot_realizations(l):\n",
    "    R = 100  # Number of realizations\n",
    "    w_LS, w_lambda = get_realizations_of_ls_estimates(R, [0., l])\n",
    "    avg_LS = w_LS.mean(axis=0)\n",
    "    avg_lambda = w_lambda.mean(axis=0)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(w_LS[:, 0], w_LS[:, 1], label='Unregularized', marker='x')\n",
    "    plt.scatter(w_lambda[:, 0], w_lambda[:, 1], label=r'Regularized ($\\lambda = {:.1f}$)'.format(l), marker='x')\n",
    "    plt.scatter([1], [1], label='Truth', color='black')\n",
    "    plt.xlabel(r'$w_0$')\n",
    "    plt.ylabel(r'$w_1$')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlim(0, 1.2)\n",
    "    plt.ylim(0, 1.2)\n",
    "    plt.title(\"Samples of Estimators\")\n",
    "    plt.grid()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter([1], [1], label='Truth', color='black')\n",
    "    plt.scatter([avg_LS[0]], [avg_LS[1]], marker='x', label='Unregularized')\n",
    "    plt.scatter([avg_lambda[0]], [avg_lambda[1]], marker='x', label=r'Regularized ($\\lambda = {:.1f}$)'.format(l))\n",
    "    plt.title('Average estimates')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "if D == 2:\n",
    "    plot_realizations(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unregularized version is unbiased, but has a larger variance than the regularized version. Since the MSE is the sum of these two terms, there seems to be a trade-off appearing.\n",
    "\n",
    "We can use the following formulas to compute the bias, variance, and $\\mathrm{MSE}$ of a linear estimator of the form $w = C\\cdot Y$: $$\\begin{aligned}\n",
    "    \\mathbb{E}[w - \\bar w] &= (C\\cdot\\Phi - I) \\bar w,\\\\\n",
    "    \\mathrm{Var}[w] &= \\sigma^2 C\\cdot C^\\top\\\\\n",
    "    \\mathrm{MSE}[w] &= \\lVert\\mathbb{E}[w - \\bar w]\\rVert^2 + \\mathrm{Tr[\\mathrm{Var}[w]]}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_variance(regularizers):\n",
    "    bias = np.zeros_like(regularizers, dtype=float)\n",
    "    vars = np.zeros_like(regularizers, dtype=float)\n",
    "    I = np.eye(D)\n",
    "    for i, reg in enumerate(regularizers):\n",
    "        C = np.linalg.inv(PhiTPhi + reg * I) @ Phi.T\n",
    "        bias[i] = np.linalg.norm((C @ Phi - I) @ wbar)**2\n",
    "        vars[i] = sigma2 * np.trace(C @ C.T)\n",
    "    return bias, vars\n",
    "\n",
    "regularizers = np.linspace(0, 2, 100)\n",
    "bias, var = get_bias_variance(regularizers)\n",
    "mse = bias + var\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(regularizers, mse - min(var), label=r'$\\mathrm{MSE}$')\n",
    "plt.plot(regularizers, bias, label=r'$(\\mathrm{Bias})^2$')\n",
    "plt.plot(regularizers, var-min(var), label=r'$\\mathrm{Variance}$')\n",
    "plt.xlabel(r'$\\lambda$')\n",
    "plt.ylabel('Error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "_=plt.title('Bias, variance, and MSE (vertically shifted)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see on this plot that a little of regularization decreases the $\\mathrm{MSE}$ of the estimator.\n",
    "We can plot the estimations of the regularized estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse_ind = np.argmin(mse)\n",
    "best_reg = regularizers[best_mse_ind]\n",
    "plot_realizations(best_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('bpa2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "85b74e45f3169d6039abd91b8eff61b3ebc8a01e856c123760a52652b4514477"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
